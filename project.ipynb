{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":332363,"sourceType":"modelInstanceVersion","modelInstanceId":278601,"modelId":299505}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Subset\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nimport torch.nn.utils.spectral_norm as spectral_norm\nimport os\nimport numpy as np\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandbpass\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch-fidelity","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.login(key=secret_value_0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BottleneckResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, downsample=False, upsample=False, spectral=False):\n        super(BottleneckResBlock, self).__init__()\n        mid_channels = in_channels // 4\n\n        self.learned_shortcut = (in_channels != out_channels) or downsample or upsample\n        self.downsample = downsample\n        self.upsample = upsample\n\n        def conv3x3(in_ch, out_ch):\n            conv = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n            return spectral_norm(conv) if spectral else conv\n\n        def conv1x1(in_ch, out_ch):\n            conv = nn.Conv2d(in_ch, out_ch, 1, 1, 0)\n            return spectral_norm(conv) if spectral else conv\n\n        self.conv1 = conv1x1(in_channels, mid_channels)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        self.conv2 = conv3x3(mid_channels, mid_channels)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        self.conv3 = conv1x1(mid_channels, out_channels)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if self.learned_shortcut:\n            self.shortcut = conv1x1(in_channels, out_channels)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        if self.upsample:\n            out = F.interpolate(out, scale_factor=2)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = self.conv3(F.relu(self.bn3(out)))\n        if self.downsample:\n            out = F.avg_pool2d(out, 2)\n\n        shortcut = self.shortcut(x)\n        if self.upsample:\n            shortcut = F.interpolate(shortcut, scale_factor=2)\n        if self.downsample:\n            shortcut = F.avg_pool2d(shortcut, 2)\n\n        return out + shortcut\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ConditionalBatchNorm2d(nn.Module):\n    def __init__(self, num_features, embedding_dim):\n        super(ConditionalBatchNorm2d, self).__init__()\n        self.bn = nn.BatchNorm2d(num_features, affine=False)\n        self.gamma = nn.Linear(embedding_dim, num_features)\n        self.beta = nn.Linear(embedding_dim, num_features)\n\n    def forward(self, x, y_embed):\n        out = self.bn(x)\n        gamma = self.gamma(y_embed).unsqueeze(2).unsqueeze(3)\n        beta = self.beta(y_embed).unsqueeze(2).unsqueeze(3)\n        out = gamma * out + beta\n        return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResBlockG(nn.Module):\n    def __init__(self, in_channels, out_channels, embedding_dim):\n        super(ResBlockG, self).__init__()\n        self.cbn1 = ConditionalBatchNorm2d(in_channels, embedding_dim)\n        self.cbn2 = ConditionalBatchNorm2d(out_channels, embedding_dim)\n        self.relu = nn.ReLU(inplace=False)\n        self.upsample = nn.Upsample(scale_factor=2)\n        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, 3, padding=1))\n        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_channels, out_channels, 3, padding=1))\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Upsample(scale_factor=2),\n                nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, 1))\n            )\n        else:\n            self.shortcut = nn.Upsample(scale_factor=2)\n\n    def forward(self, x, y_embed):\n        out = self.cbn1(x, y_embed)\n        out = self.relu(out)\n        out = self.upsample(out)\n        out = self.conv1(out)\n        out = self.cbn2(out, y_embed)\n        out = self.relu(out)\n        out = self.conv2(out)\n        shortcut = self.shortcut(x)\n        return out + shortcut\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BigGANDeepLiteGenerator(nn.Module):\n    def __init__(self, latent_dim, num_classes, embedding_dim=128, ch=64):\n        super(BigGANDeepLiteGenerator, self).__init__()\n        self.latent_dim = latent_dim\n        self.num_classes = num_classes\n        self.embedding_dim = embedding_dim\n        self.init_size = 4\n        self.project = nn.Linear(latent_dim, (ch * 16) * self.init_size * self.init_size)\n        self.label_embedding = nn.Embedding(num_classes, embedding_dim)\n\n        self.resblock1 = ResBlockG(ch * 16, ch * 8, embedding_dim)\n        self.resblock2 = ResBlockG(ch * 8, ch * 4, embedding_dim)\n        self.resblock3 = ResBlockG(ch * 4, ch * 2, embedding_dim)\n\n        self.bn = nn.BatchNorm2d(ch * 2)\n        self.relu = nn.ReLU(inplace=False)\n        self.final_conv = nn.utils.spectral_norm(nn.Conv2d(ch * 2, 3, 3, padding=1))\n        self.tanh = nn.Tanh()\n\n    def forward(self, z, labels):\n        y_embed = self.label_embedding(labels)\n        out = self.project(z).view(z.size(0), -1, self.init_size, self.init_size)\n\n        out = self.resblock1(out, y_embed)\n        out = self.resblock2(out, y_embed)\n        out = self.resblock3(out, y_embed)\n\n        out = self.relu(self.bn(out))\n        out = self.final_conv(out)\n        out = self.tanh(out)\n        return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass ResBlockD(nn.Module):\n    def __init__(self, in_channels, out_channels, downsample=True):\n        super(ResBlockD, self).__init__()\n        self.downsample = downsample\n        self.learned_shortcut = (in_channels != out_channels) or downsample\n\n        self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n        self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n        self.activation = nn.ReLU(inplace=False)\n        self.avgpool = nn.AvgPool2d(2)\n\n        if self.learned_shortcut:\n            self.shortcut = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0))\n\n    def forward(self, x):\n        residual = x\n\n        out = self.activation(x)\n        out = self.conv1(out)\n        out = self.activation(out)\n        out = self.conv2(out)\n        if self.downsample:\n            out = self.avgpool(out)\n\n        if self.learned_shortcut:\n            residual = self.shortcut(residual)\n            if self.downsample:\n                residual = self.avgpool(residual)\n\n        return out + residual","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BigGANDeepLiteDiscriminator(nn.Module):\n    def __init__(self, num_classes=10, channels=64, use_augself=False, num_aug_types=4):\n        super(BigGANDeepLiteDiscriminator, self).__init__()\n        self.use_augself = use_augself\n\n        self.block1 = ResBlockD(3, channels, downsample=True)\n        self.block2 = ResBlockD(channels, channels * 2, downsample=True)\n        self.block3 = ResBlockD(channels * 2, channels * 4, downsample=True)\n        self.block4 = ResBlockD(channels * 4, channels * 8, downsample=True)\n        self.block5 = ResBlockD(channels * 8, channels * 16, downsample=False)\n\n        self.activation = nn.ReLU(inplace=False)\n        self.linear = spectral_norm(nn.Linear(channels * 16, 1))\n        self.embed = spectral_norm(nn.Embedding(num_classes, channels * 16))\n\n        # Augmentation classifier (only used if use_augself=True)\n        if self.use_augself:\n            self.aug_classifier = nn.Linear(channels * 16, num_aug_types)\n\n    def forward(self, x, y, return_aug_logits=False):\n        out = self.block1(x)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.block4(out)\n        out = self.block5(out)\n\n        out = self.activation(out)\n        pooled = torch.sum(out, dim=(2, 3))  # Global sum pooling\n        out_linear = self.linear(pooled)\n\n        proj = torch.sum(pooled * self.embed(y), dim=1, keepdim=True)\n        output = out_linear + proj\n\n        if self.use_augself and return_aug_logits:\n            aug_logits = self.aug_classifier(pooled)\n            return output, aug_logits\n        return output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_root = '/kaggle/working/'\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrain_dataset = torchvision.datasets.CIFAR10(root=data_root, train=True, download=True, transform=transform)\ntest_dataset = torchvision.datasets.CIFAR10(root=data_root, train=False, download=True, transform=transform)\ndef get_partial_cifar10(split='train', percentage=0.1, seed=42, transform=None):\n    # Load the full CIFAR-10 dataset\n    dataset = torchvision.datasets.CIFAR10(\n        root='./data',\n        train=(split=='train'),\n        transform=transform,\n        download=True\n    )\n\n    # Get class-wise indices\n    np.random.seed(seed)\n    targets = np.array(dataset.targets)\n    selected_indices = []\n\n    for cls in range(10):  # one for each class\n        cls_indices = np.where(targets == cls)[0]\n        num_samples = int(len(cls_indices) * percentage)\n        chosen = np.random.choice(cls_indices, num_samples, replace=False)\n        selected_indices.extend(chosen)\n\n    return Subset(dataset, selected_indices)\ntrain_dataset_10 = get_partial_cifar10(split='train', percentage=0.1, transform=transform)\ntrain_loader_10 = DataLoader(train_dataset_10, batch_size=128, shuffle=True,num_workers = 2)\ntrain_dataset_20 = get_partial_cifar10(split='train', percentage=0.2, transform=transform)\ntrain_loader_20 = DataLoader(train_dataset_10, batch_size=128, shuffle=True,num_workers = 2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_dataset,\n    batch_size=128,\n    shuffle=True,\n    pin_memory=True,\n    num_workers = 2# Optional: can help on GPU\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run = wandb.init(\n    entity=\"Hexager-manipal\",\n    # Set the wandb project where this run will be logged.\n    project=\"Big-Gan-similar\",\n    # Track hyperparameters and run metadata.\n    config={\n        \"learning_rate\": 2e-4,\n        \"architecture\": \"BIG-GAN-deep-lite-simplified w/o enhancements\",\n        \"dataset\": \"CIFAR-10\",\n        \"epochs\": 20\n    },\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"AUG_TYPES = {\n    0: transforms.RandomRotation(degrees=15),\n    1: transforms.ColorJitter(0.4, 0.4, 0.4),\n    2: transforms.RandomHorizontalFlip(p=1.0),\n    3: transforms.RandomGrayscale(p=1.0)\n}\nNUM_AUGS = len(AUG_TYPES)\n\ndef apply_aug_batch(batch, aug_labels):\n    # Apply the corresponding augmentation to each image\n    return torch.stack([AUG_TYPES[aug.item()](img.cpu()).to(img.device) for aug, img in zip(aug_labels, batch)])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_checkpoint(generator, discriminator, g_optimizer, d_optimizer,\n                    epoch, step,  path=\"checkpoints\", filename=\"last.pth\"):\n    os.makedirs(path, exist_ok=True)\n    \n    checkpoint = {\n        \"generator\": generator.state_dict(),\n        \"discriminator\": discriminator.state_dict(),\n        \"g_optimizer\": g_optimizer.state_dict(),\n        \"d_optimizer\": d_optimizer.state_dict(),\n        \"epoch\": epoch,\n        \"step\": step,\n        #\"best_fid\": best_fid,\n    }\n    \n    torch.save(checkpoint, os.path.join(path, filename))\n    #if is_best:\n        #torch.save(checkpoint, os.path.join(path, \"best.pth\"))\n\n\ndef load_checkpoint(generator, discriminator, g_optimizer, d_optimizer, path=\"checkpoints/last.pth\"):\n    checkpoint = torch.load(path)\n    generator.load_state_dict(checkpoint[\"generator\"])\n    discriminator.load_state_dict(checkpoint[\"discriminator\"])\n    g_optimizer.load_state_dict(checkpoint[\"g_optimizer\"])\n    d_optimizer.load_state_dict(checkpoint[\"d_optimizer\"])\n    return checkpoint[\"epoch\"], checkpoint[\"step\"] #checkpoint[\"best_fid\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torchvision.utils import make_grid, save_image\nfrom tqdm import tqdm\n\ndef hinge_discriminator_loss(D_real, D_fake):\n    return torch.mean(F.relu(1. - D_real)) + torch.mean(F.relu(1. + D_fake))\n\ndef hinge_generator_loss(D_fake):\n    return -torch.mean(D_fake)\n\ndef sample_latent(batch_size, z_dim, num_classes, device):\n    z = torch.randn(batch_size, z_dim, device=device)\n    y = torch.randint(0, num_classes, (batch_size,), device=device)\n    return z, y\n\ndef train(\n    generator,\n    discriminator,\n    dataloader,\n    num_classes,\n    z_dim=128,\n    epochs=100,\n    lr_g=2e-4,\n    lr_d=2e-4,\n    device='cuda',\n    g_steps=1,\n    d_steps=1,\n    save_interval=5,\n    use_pseudo_aug=False,           # <â€” new flag\n    t_threshold=0.6,                # APA threshold\n    p_step=0.01,                    # APA step size\n    adjust_every=4,                  # APA adjust frequency\n    use_augself = True,  # ðŸ” Set False to disable AugSelf\n    lambda_aug = 1.0  # Weight of the contrastive loss\n\n):\n    # â€” Pseudoâ€‘Augmentation state â€”\n    pseudo_aug_prob = 0.0           # initial deception prob.\n    d_iter          = 0             # counter for D updates\n    generator = generator.to(device)\n    discriminator = discriminator.to(device)\n    opt_g = torch.optim.Adam(generator.parameters(), lr=lr_g, betas=(0.0, 0.999))\n    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.0, 0.999))\n    step = 0\n\n    for epoch in range(epochs):\n        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        for real_imgs, labels in pbar:\n            real_imgs, labels = real_imgs.to(device), labels.to(device)\n            bs = real_imgs.size(0)\n\n            # â€” Train Discriminator â€”\n            for _ in range(d_steps):\n                # 1) generate fake and update buffer\n                z, y = sample_latent(bs, z_dim, num_classes, device)\n                fake_imgs = generator(z, y).detach()\n\n                # 2) mix pseudo into real (only if APA enabled)\n                if use_pseudo_aug and pseudo_aug_prob > 0 and epoch>4:\n                    mask = (torch.rand(bs,1,1,1,device=device) < pseudo_aug_prob).float()\n                    real_mixed = real_imgs * (1 - mask) + fake_imgs * mask\n                else:\n                    real_mixed = real_imgs\n                if use_augself:\n                    aug_labels = torch.randint(0, NUM_AUGS, (bs,), device=device)\n                    aug_imgs = apply_aug_batch(real_mixed, aug_labels)\n                    D_real_aug, aug_logits = discriminator(aug_imgs, labels, return_aug_logits=True)\n                    loss_aug = torch.clamp(F.cross_entropy(aug_logits, aug_labels), max=5.0)\n                else:\n                    loss_aug = torch.tensor(0.0, device=device)\n                # 3) forward\n                D_real = discriminator(real_mixed, labels, return_aug_logits=False)\n                D_fake = discriminator(fake_imgs, y, return_aug_logits=False)\n\n                loss_adv = hinge_discriminator_loss(D_real, D_fake)\n                loss_d = loss_adv + lambda_aug * loss_aug\n                # 4) backward + step\n                opt_d.zero_grad()\n                loss_d.backward()\n                opt_d.step()\n\n                # 5) adapt pseudo_aug_prob if APA enabled\n                if use_pseudo_aug and epoch>4:\n                    d_iter += 1\n                    if d_iter % adjust_every == 0:\n                        lambda_r = float(torch.mean(torch.sign(D_real)).item())\n                        if lambda_r > t_threshold:\n                            pseudo_aug_prob = min(pseudo_aug_prob + p_step, 1.0)\n                        else:\n                            pseudo_aug_prob = max(pseudo_aug_prob - p_step, 0.0)\n\n            # â€” Train Generator â€”\n            for _ in range(g_steps):\n                z, y = sample_latent(bs, z_dim, num_classes, device)\n                fake_imgs = generator(z, y)\n                D_fake = discriminator(fake_imgs, y)\n                loss_g = hinge_generator_loss(D_fake)\n\n                opt_g.zero_grad()\n                loss_g.backward()\n                opt_g.step()\n\n            # â€” Logging & checkpointing â€”\n            postfix = {\n                \"d_loss\": loss_d.item(),\n                \"g_loss\": loss_g.item(),\n            }\n            if use_pseudo_aug and epoch>4:\n                postfix[\"pseudo_aug_prob\"] = pseudo_aug_prob\n            pbar.set_postfix(postfix)\n            step += 1\n            if step % 50 == 0:\n                log_dict = {\n                    \"d_loss\": loss_d.item(),\n                    \"g_loss\": loss_g.item(),\n                    \"epoch\": epoch,\n                    \"step\": step\n                }\n                if use_pseudo_aug and epoch>4:\n                    log_dict[\"pseudo_aug_prob\"] = pseudo_aug_prob \n                if use_augself:\n                    log_dict[\"aug_loss\"] = loss_aug\n                wandb.log(log_dict)\n                \n\n        save_checkpoint(generator, discriminator, opt_g, opt_d, epoch, step)\n        # ------------------\n        # Save Images\n        # ------------------\n        if epoch % 5 == 0:\n            generator.eval()\n            with torch.no_grad():\n                z = torch.randn(64, z_dim, device=device)\n                labels = torch.randint(0, num_classes, (64,), device=device)\n                fakes = generator(z, labels)\n                grid = torchvision.utils.make_grid(fakes, nrow=8, normalize=True)\n                wandb.log({\"Generated Images\": [wandb.Image(grid, caption=f\"Epoch {epoch}\")]})\n            generator.train()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"use_augself = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator = BigGANDeepLiteGenerator(latent_dim=128, num_classes=10)\ndiscriminator = BigGANDeepLiteDiscriminator(num_classes=10, use_augself=use_augself)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train(generator, discriminator, train_loader_20, num_classes=10, epochs=50, use_pseudo_aug=True, use_augself=use_augself)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator = BigGANDeepLiteGenerator(latent_dim=128, num_classes=10)\ngenerator.load_state_dict(torch.load(\"/kaggle/working/checkpoints/last.pth\")[\"generator\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.datasets import CIFAR10\nfrom torchvision import transforms\nimport torch\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),                   # [0,1]\n    transforms.Normalize((0.5, 0.5, 0.5),     # back to [-1, 1]\n                         (0.5, 0.5, 0.5))\n])\n\nreal_dataset = CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\nreal_images = torch.stack([real_dataset[i][0] for i in range(5000)])  # shape: [5000, 3, 32, 32]\ngenerator.eval().to(\"cuda\")\nlatent_dim = 128\nnum_classes = 10\nbatch_size = 64\nfakes = []\n\nwith torch.no_grad():\n    for _ in range(5000 // batch_size):\n        z = torch.randn(batch_size, latent_dim).to(\"cuda\")\n        y = torch.randint(0, num_classes, (batch_size,), device=\"cuda\")\n        out = generator(z, y)\n        fakes.append(out.cpu())  # offload to CPU to save VRAM\n\nfakes_tensor = torch.cat(fakes, dim=0)  # [5000, 3, 32, 32]\n\ndef denorm(x):\n    return (x * 0.5 + 0.5).clamp(0, 1)\n\nreal_images = denorm(real_images)\nfakes_tensor = denorm(fakes_tensor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchmetrics.image.fid import FrechetInceptionDistance\n\nfid = FrechetInceptionDistance(feature=2048, normalize=True).to(\"cuda\")\n\n# Feed real images\nfor i in range(0, 10000, batch_size):\n    real_batch = real_images[i:i+batch_size].to(\"cuda\")\n    if real_batch.size(0) == 0:\n        continue\n    fid.update(real_batch, real=True)\n\n# Feed fake images\nfor i in range(0, fakes_tensor.size(0), batch_size):\n    fake_batch = fakes_tensor[i:i+batch_size]\n    if fake_batch.size(0) == 0:\n        continue  # skip empty batches just in case\n    fid.update(fake_batch.to(\"cuda\"), real=False)\n\n\n# Compute score\nscore = fid.compute().item()\nprint(f\"âœ… Final FID Score: {score:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchmetrics.image.inception import InceptionScore\n\n# Normalize fake images to [0, 1] if not already\nfake_imgs = fakes_tensor.clone()  # shape: [5000, 3, 32, 32]\nfake_imgs = fake_imgs.clamp(0, 1)\n\n# Create IS object\nis_metric = InceptionScore(normalize=True, splits=10).to(\"cuda\")\n\n# Feed fake images in batches\nbatch_size = 64\nfor i in range(0, fake_imgs.size(0), batch_size):\n    batch = fake_imgs[i:i+batch_size]\n    if batch.size(0) == 0:\n        continue\n    is_metric.update(batch.to(\"cuda\"))\n\n# Compute IS\nscore, std = is_metric.compute()\nprint(f\"âœ… Inception Score: {score:.2f} Â± {std:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}